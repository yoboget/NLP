{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRnDnCW-Z7qv"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "data = open('songs.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9vH8Y59ajYL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yboge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\yboge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 100)           321100    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1605)              162105    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3211)              5156866   \n",
      "=================================================================\n",
      "Total params: 6,101,671\n",
      "Trainable params: 6,101,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIg2f1HBxqof"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yboge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "15462/15462 [==============================] - 100s 6ms/sample - loss: 6.9156 - acc: 0.0196s - loss: 6.9156 - acc: 0.019\n",
      "Epoch 2/100\n",
      "15462/15462 [==============================] - 119s 8ms/sample - loss: 6.4999 - acc: 0.0220\n",
      "Epoch 3/100\n",
      "15462/15462 [==============================] - 118s 8ms/sample - loss: 6.3896 - acc: 0.0236\n",
      "Epoch 4/100\n",
      "15462/15462 [==============================] - 92s 6ms/sample - loss: 6.2690 - acc: 0.02993s - loss:\n",
      "Epoch 5/100\n",
      "15462/15462 [==============================] - 89s 6ms/sample - loss: 6.1776 - acc: 0.0347\n",
      "Epoch 6/100\n",
      "15462/15462 [==============================] - 91s 6ms/sample - loss: 6.0998 - acc: 0.03850s - loss: 6.1000 - acc: \n",
      "Epoch 7/100\n",
      "15462/15462 [==============================] - 95s 6ms/sample - loss: 6.0189 - acc: 0.0409\n",
      "Epoch 8/100\n",
      "15462/15462 [==============================] - 101s 7ms/sample - loss: 5.9328 - acc: 0.0421\n",
      "Epoch 9/100\n",
      "15462/15462 [==============================] - 100s 6ms/sample - loss: 5.8272 - acc: 0.0495\n",
      "Epoch 10/100\n",
      "15462/15462 [==============================] - 98s 6ms/sample - loss: 5.7181 - acc: 0.05502s - loss: 5.7179\n",
      "Epoch 11/100\n",
      "15462/15462 [==============================] - 111s 7ms/sample - loss: 5.5951 - acc: 0.0656s - loss: 5.5953 - acc: 0.06\n",
      "Epoch 12/100\n",
      "15462/15462 [==============================] - 133s 9ms/sample - loss: 5.4728 - acc: 0.0726\n",
      "Epoch 13/100\n",
      "15462/15462 [==============================] - 115s 7ms/sample - loss: 5.3553 - acc: 0.0781\n",
      "Epoch 14/100\n",
      "15462/15462 [==============================] - 94s 6ms/sample - loss: 5.2458 - acc: 0.0828\n",
      "Epoch 15/100\n",
      "15462/15462 [==============================] - 101s 7ms/sample - loss: 5.1341 - acc: 0.0944\n",
      "Epoch 16/100\n",
      "15462/15462 [==============================] - 111s 7ms/sample - loss: 5.0230 - acc: 0.10369s - loss: 5 - ETA: 5s - los\n",
      "Epoch 17/100\n",
      "15462/15462 [==============================] - 99s 6ms/sample - loss: 4.9147 - acc: 0.1066\n",
      "Epoch 18/100\n",
      "15462/15462 [==============================] - 91s 6ms/sample - loss: 4.8106 - acc: 0.11764s - lo\n",
      "Epoch 19/100\n",
      "15462/15462 [==============================] - 88s 6ms/sample - loss: 4.7091 - acc: 0.12922s - loss: 4.7077 \n",
      "Epoch 20/100\n",
      "15462/15462 [==============================] - 94s 6ms/sample - loss: 4.5969 - acc: 0.1356\n",
      "Epoch 21/100\n",
      "15462/15462 [==============================] - 91s 6ms/sample - loss: 4.4919 - acc: 0.14750s - loss: 4.4907 - acc: \n",
      "Epoch 22/100\n",
      "15462/15462 [==============================] - 92s 6ms/sample - loss: 4.3841 - acc: 0.15883s - loss: 4.3827 - acc: 0. - ETA: 2s - loss: 4.3825\n",
      "Epoch 23/100\n",
      "15462/15462 [==============================] - 98s 6ms/sample - loss: 4.2890 - acc: 0.16970s - loss: 4.2896 - acc: 0.169\n",
      "Epoch 24/100\n",
      "15462/15462 [==============================] - 104s 7ms/sample - loss: 4.1809 - acc: 0.1835s - loss: 4.175\n",
      "Epoch 25/100\n",
      "15462/15462 [==============================] - 101s 7ms/sample - loss: 4.0815 - acc: 0.1956\n",
      "Epoch 26/100\n",
      "15462/15462 [==============================] - 106s 7ms/sample - loss: 3.9647 - acc: 0.2128\n",
      "Epoch 27/100\n",
      "15462/15462 [==============================] - 110s 7ms/sample - loss: 3.8665 - acc: 0.2238\n",
      "Epoch 28/100\n",
      "15462/15462 [==============================] - 94s 6ms/sample - loss: 3.7699 - acc: 0.2405\n",
      "Epoch 29/100\n",
      "15462/15462 [==============================] - 96s 6ms/sample - loss: 3.6726 - acc: 0.2621\n",
      "Epoch 30/100\n",
      "15462/15462 [==============================] - 176s 11ms/sample - loss: 3.5826 - acc: 0.2769s - loss: 3.5744 - acc:  - ETA: 10s\n",
      "Epoch 31/100\n",
      "15462/15462 [==============================] - 116s 7ms/sample - loss: 3.4815 - acc: 0.3016\n",
      "Epoch 32/100\n",
      "15462/15462 [==============================] - 121s 8ms/sample - loss: 3.3927 - acc: 0.3238\n",
      "Epoch 33/100\n",
      "15462/15462 [==============================] - 104s 7ms/sample - loss: 3.3070 - acc: 0.3400\n",
      "Epoch 34/100\n",
      "15462/15462 [==============================] - 101s 7ms/sample - loss: 3.2151 - acc: 0.3643\n",
      "Epoch 35/100\n",
      "15462/15462 [==============================] - 104s 7ms/sample - loss: 3.1395 - acc: 0.3763\n",
      "Epoch 36/100\n",
      "15462/15462 [==============================] - 116s 8ms/sample - loss: 3.0577 - acc: 0.3945s - loss: - ETA: 3s - loss: 3.0517\n",
      "Epoch 37/100\n",
      "15462/15462 [==============================] - 96s 6ms/sample - loss: 2.9981 - acc: 0.4124\n",
      "Epoch 38/100\n",
      "15462/15462 [==============================] - 108s 7ms/sample - loss: 2.9185 - acc: 0.4272s\n",
      "Epoch 39/100\n",
      "15462/15462 [==============================] - 116s 7ms/sample - loss: 2.8464 - acc: 0.4443\n",
      "Epoch 40/100\n",
      "15462/15462 [==============================] - 105s 7ms/sample - loss: 2.7788 - acc: 0.4621\n",
      "Epoch 41/100\n",
      "15462/15462 [==============================] - 108s 7ms/sample - loss: 2.7207 - acc: 0.4736s \n",
      "Epoch 42/100\n",
      "15462/15462 [==============================] - 101s 7ms/sample - loss: 2.6500 - acc: 0.4867 ETA: 2s - loss: 2.6499\n",
      "Epoch 43/100\n",
      "15462/15462 [==============================] - 99s 6ms/sample - loss: 2.5831 - acc: 0.5017\n",
      "Epoch 44/100\n",
      "15462/15462 [==============================] - 102s 7ms/sample - loss: 2.5226 - acc: 0.5180\n",
      "Epoch 45/100\n",
      "15462/15462 [==============================] - 97s 6ms/sample - loss: 2.4839 - acc: 0.5239\n",
      "Epoch 46/100\n",
      "15462/15462 [==============================] - 100s 6ms/sample - loss: 2.4221 - acc: 0.5418\n",
      "Epoch 47/100\n",
      "15462/15462 [==============================] - 104s 7ms/sample - loss: 2.3688 - acc: 0.5546s - loss: 2.3697 - acc: 0.5\n",
      "Epoch 48/100\n",
      "15462/15462 [==============================] - 108s 7ms/sample - loss: 2.3247 - acc: 0.5622\n",
      "Epoch 49/100\n",
      "15462/15462 [==============================] - 107s 7ms/sample - loss: 2.2574 - acc: 0.5777\n",
      "Epoch 50/100\n",
      "15462/15462 [==============================] - 123s 8ms/sample - loss: 2.2237 - acc: 0.5843\n",
      "Epoch 51/100\n",
      "15462/15462 [==============================] - 112s 7ms/sample - loss: 2.1716 - acc: 0.5975\n",
      "Epoch 52/100\n",
      "15462/15462 [==============================] - 102s 7ms/sample - loss: 2.1366 - acc: 0.6057\n",
      "Epoch 53/100\n",
      "15462/15462 [==============================] - 111s 7ms/sample - loss: 2.0802 - acc: 0.6180\n",
      "Epoch 54/100\n",
      "15462/15462 [==============================] - 108s 7ms/sample - loss: 2.0434 - acc: 0.6240\n",
      "Epoch 55/100\n",
      "15462/15462 [==============================] - 102s 7ms/sample - loss: 2.0049 - acc: 0.6370s - ETA: 2s - loss: 2.0035 -\n",
      "Epoch 56/100\n",
      "15462/15462 [==============================] - 104s 7ms/sample - loss: 1.9777 - acc: 0.6434\n",
      "Epoch 57/100\n",
      "15462/15462 [==============================] - 102s 7ms/sample - loss: 1.9212 - acc: 0.65540s - loss: 1. - ETA:\n",
      "Epoch 58/100\n",
      "15462/15462 [==============================] - 104s 7ms/sample - loss: 1.8973 - acc: 0.65751s - loss -\n",
      "Epoch 59/100\n",
      "15462/15462 [==============================] - 103s 7ms/sample - loss: 1.8676 - acc: 0.6640\n",
      "Epoch 60/100\n",
      "15462/15462 [==============================] - 102s 7ms/sample - loss: 1.8411 - acc: 0.6724\n",
      "Epoch 61/100\n",
      "15462/15462 [==============================] - 105s 7ms/sample - loss: 1.7890 - acc: 0.6833\n",
      "Epoch 62/100\n",
      "15462/15462 [==============================] - 106s 7ms/sample - loss: 1.7597 - acc: 0.6900\n",
      "Epoch 63/100\n",
      "15462/15462 [==============================] - 107s 7ms/sample - loss: 1.7321 - acc: 0.6945\n",
      "Epoch 64/100\n",
      "15462/15462 [==============================] - 106s 7ms/sample - loss: 1.7094 - acc: 0.6973\n",
      "Epoch 65/100\n",
      "15462/15462 [==============================] - 104s 7ms/sample - loss: 1.6875 - acc: 0.6971\n",
      "Epoch 66/100\n",
      "15462/15462 [==============================] - 128s 8ms/sample - loss: 1.6723 - acc: 0.7057\n",
      "Epoch 67/100\n",
      "15462/15462 [==============================] - 127s 8ms/sample - loss: 1.6440 - acc: 0.7083\n",
      "Epoch 68/100\n",
      "15462/15462 [==============================] - 126s 8ms/sample - loss: 1.6013 - acc: 0.7222\n",
      "Epoch 69/100\n",
      "15462/15462 [==============================] - 136s 9ms/sample - loss: 1.5978 - acc: 0.7200\n",
      "Epoch 70/100\n",
      "15462/15462 [==============================] - 129s 8ms/sample - loss: 1.5708 - acc: 0.7244s - loss: 1.5675 -  - ETA: 6s - l\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15462/15462 [==============================] - 100s 6ms/sample - loss: 1.5358 - acc: 0.7345s - loss: 1.5313 - acc: 0 - ETA: 1s - loss: 1.5333 - a\n",
      "Epoch 72/100\n",
      "15462/15462 [==============================] - 99s 6ms/sample - loss: 1.5120 - acc: 0.7392\n",
      "Epoch 73/100\n",
      "15462/15462 [==============================] - 114s 7ms/sample - loss: 1.4908 - acc: 0.7418\n",
      "Epoch 74/100\n",
      "15462/15462 [==============================] - 124s 8ms/sample - loss: 1.4751 - acc: 0.7445\n",
      "Epoch 75/100\n",
      "15462/15462 [==============================] - 119s 8ms/sample - loss: 1.4575 - acc: 0.7478\n",
      "Epoch 76/100\n",
      "15462/15462 [==============================] - 111s 7ms/sample - loss: 1.4357 - acc: 0.7513\n",
      "Epoch 77/100\n",
      "15462/15462 [==============================] - 99s 6ms/sample - loss: 1.4242 - acc: 0.7501\n",
      "Epoch 78/100\n",
      "15462/15462 [==============================] - 101s 7ms/sample - loss: 1.4261 - acc: 0.7529\n",
      "Epoch 79/100\n",
      "15462/15462 [==============================] - 118s 8ms/sample - loss: 1.3840 - acc: 0.7643\n",
      "Epoch 80/100\n",
      "15462/15462 [==============================] - 111s 7ms/sample - loss: 1.3536 - acc: 0.7682\n",
      "Epoch 81/100\n",
      "15462/15462 [==============================] - 102s 7ms/sample - loss: 1.3359 - acc: 0.7727s - loss: 1.3362 - a\n",
      "Epoch 82/100\n",
      "15462/15462 [==============================] - 1911s 124ms/sample - loss: 1.3284 - acc: 0.7735s - \n",
      "Epoch 83/100\n",
      "15462/15462 [==============================] - 93s 6ms/sample - loss: 1.3213 - acc: 0.76935s \n",
      "Epoch 84/100\n",
      "15462/15462 [==============================] - 98s 6ms/sample - loss: 1.3238 - acc: 0.77020s - loss: 1.3246 - acc: 0\n",
      "Epoch 85/100\n",
      "15462/15462 [==============================] - 103s 7ms/sample - loss: 1.3023 - acc: 0.7758\n",
      "Epoch 86/100\n",
      "15462/15462 [==============================] - 96s 6ms/sample - loss: 1.2874 - acc: 0.77625s -\n",
      "Epoch 87/100\n",
      "15462/15462 [==============================] - 98s 6ms/sample - loss: 1.2593 - acc: 0.7851\n",
      "Epoch 88/100\n",
      "15462/15462 [==============================] - 107s 7ms/sample - loss: 1.2430 - acc: 0.7873s - loss: 1.2349 - acc: 0\n",
      "Epoch 89/100\n",
      "15462/15462 [==============================] - 104s 7ms/sample - loss: 1.2418 - acc: 0.7865\n",
      "Epoch 90/100\n",
      "15462/15462 [==============================] - 101s 7ms/sample - loss: 1.2241 - acc: 0.7900\n",
      "Epoch 91/100\n",
      "15462/15462 [==============================] - 101s 7ms/sample - loss: 1.2137 - acc: 0.7896s - loss: 1.2117 - ETA: 2s - loss: 1.2129\n",
      "Epoch 92/100\n",
      "15462/15462 [==============================] - 98s 6ms/sample - loss: 1.2121 - acc: 0.78865s\n",
      "Epoch 93/100\n",
      "15462/15462 [==============================] - 93s 6ms/sample - loss: 1.2055 - acc: 0.7914\n",
      "Epoch 94/100\n",
      "15462/15462 [==============================] - 93s 6ms/sample - loss: 1.1869 - acc: 0.794911s - loss: 1.1733 - ETA: - ETA: 4s - l\n",
      "Epoch 95/100\n",
      "15462/15462 [==============================] - 92s 6ms/sample - loss: 1.1820 - acc: 0.79410s - loss: 1.1819 - acc: 0\n",
      "Epoch 96/100\n",
      "15462/15462 [==============================] - 92s 6ms/sample - loss: 1.1567 - acc: 0.8006\n",
      "Epoch 97/100\n",
      "15462/15462 [==============================] - 93s 6ms/sample - loss: 1.1528 - acc: 0.7985\n",
      "Epoch 98/100\n",
      "15462/15462 [==============================] - 92s 6ms/sample - loss: 1.1433 - acc: 0.8002\n",
      "Epoch 99/100\n",
      "15462/15462 [==============================] - 91s 6ms/sample - loss: 1.1349 - acc: 0.8024\n",
      "Epoch 100/100\n",
      "15462/15462 [==============================] - 94s 6ms/sample - loss: 1.1242 - acc: 0.80311s - loss: 1.1231 - acc:\n"
     ]
    }
   ],
   "source": [
    " history = model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fXTEO3GJ282"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP_Week4_Exercise_Shakespeare_Answer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
